{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Implement advanced Transformer models using Keras. \n",
    "\n",
    "- Apply Transformers to real-world sequential data tasks. \n",
    "\n",
    "- Build, train, and evaluate Transformer models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-18.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.11.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.68.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m832.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-18.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.68.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (391 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.8/391.8 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, pyarrow, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.0 h5py-3.12.1 keras-3.6.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.13.1 protobuf-5.28.3 pyarrow-18.0.0 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m16.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m326.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.5/164.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (24.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.0 kiwisolver-1.4.7 matplotlib-3.9.2 pillow-11.0.0 pyparsing-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 03:02:30.544517: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-18 03:02:30.552396: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-18 03:02:30.558923: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-18 03:02:30.577358: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731898950.603966      82 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731898950.609856      82 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 03:02:30.631926: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Close\n",
      "0  100.993428\n",
      "1   99.773496\n",
      "2  101.395427\n",
      "3  103.196135\n",
      "4   99.731793\n",
      "(2000, 1)\n",
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "print(data.head())\n",
    "data = data[['Close']].values \n",
    "print(data.shape)\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "#Accessing the first slice\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 03:19:49.790211: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - loss: 10.9219\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.2703\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.1388\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1408\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.1197\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1359\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.1906\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0930\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1070\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0999\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0780\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0791\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1039\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0655\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0736\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0431\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0398\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0411\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0434\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f766e955450>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 386ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    " \n",
    "\n",
    "# Plot the predictions \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(data, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.show() \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - loss: 16.6517\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 303ms/step - loss: 1.1337\n",
      "Test loss: 1.637652039527893\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Define the necessary parameters \n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "dropout_layer = Dropout(rate=0.5)(flatten)\n",
    "outputs = tf.keras.layers.Dense(1)(dropout_layer) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=1, batch_size=32)\n",
    "\n",
    "# Evaluate the model \n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 622ms/step - loss: 2.3612\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 303ms/step - loss: 0.0574\n",
      "Test loss with batch size 16: 0.17822344601154327\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 1.3915\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 300ms/step - loss: 0.0176\n",
      "Test loss with batch size 64: 0.03618292883038521\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=1, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=1, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_100 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_101 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - loss: 2.2263\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 304ms/step - loss: 1.7508\n",
      "Test loss: 2.3743340969085693\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1,activation='tanh')(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=1, batch_size=32)\n",
    "\n",
    "# Evaluate the model \n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "28ac4fd81c1d713f83dcd1cdf1d3383ad25ea92873288fe9e978e9a17b314709"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
